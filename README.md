# ONNX FixedPoint proof-of-concept
A proof-of-concept for the conversion of a quantized pytorch model to an ONNX FixedPOint graph of operations

## Docker setup

### Build docker image

```
./docker.sh -c 1
```

### Run docker image

```
./docker.sh
```

## Venv setup

### Create venv

```
python -m venv onnx_tf_venv
```

### Activate venv

```
source ./onnx_tf_venv/bin/activate
```

## Training of a basic MNIST model

The first step is to instantiate and train a basic MNIST model for a few epochs.

```
python train_mnist_model.py --epochs 5
```

This should produce a model that reaches `99 %` accuracy, and save it as `mnist_cnn.pt`.

Note: the model is saved as a state_dict, so it cannot be instantiated without
the original model definition.

## Quantize a trained MNIST model

The next step is to statically quantize the float MNIST model to 8-bit.

```
python quantize_model.py --model mnist_cnn.pt \
                         --save_model mnist_cnn_quantized.pts
```

This will quantize the model to 8-bit and calibrate it, resulting in a model
with an equivalent accuracy (99 %).

Note that the model is calibrated with random inputs: calibrating it with inputs
from the dataset will likely improve the accuracy.

Note also that the quantization script does not use the default quantization
backend configuration, as it includes a fused operator that is not supported
by ONNX (the default config is therefore edited to remove the transformation
producing the fused operator).

The quantized model is saved as a `mnist_cnn_quantized.pts` TorchScript model
that can be used for inference or conversion to ONNX.

## Convert the quantized model to ONNX

The conversion to ONNX is rather straightforward. The only caveat is that the
batch size must be specified at conversion, as it is not dynamic.

```
python torch_to_onnx.py --model mnist_cnn_quantized.pts \
                        --save_model mnist_cnn_quantized.onnx \
                        --test-batch-size 1000
```

Once converted, the model can be sued for inference on the MNIST test set.

```
python onnx_mnist.py --model mnist_cnn_quantized.onnx \
                     --test-batch-size 1000
```

## Modify the ONNX quantized model to use integer-only operation

### Sanitization

The quantized model generated by pytorch contains spurious operations.

The first transformation that needs to be applied is to sanitize the graph by
applying a built-in onnx_graphsurgeon transformation to fold constant nodes and
remove some of them.

```
python onnx_transforms.py --model mnist_cnn_quantized.onnx \
                          --save_model mnist_cnn_sanitized.onnx
```

Note: this is strictly equivalent to calling the NVIDIA `polygraphy` command-line tool
with the `surgeon sanitize` command.

### Add preprocessing to the graph

Since we only want integer operations, we need to integrate the uint8 inputs preprocessing in the graph.

The MNIST preprocessing applies the following Normalization:

```
float_inputs = (inputs / 255 - mean) / std

With:
 mean = 0.1307
 std = 0.3081
```

We can replace it with a `DequantizeLinear` operation:

```
float_inputs = scale * (inputs - zero_point)

With:
  scale = 1 / (255 * std) = 0.0127
  zero_point = round(255 * mean) = 33
```

This transformation is available through the new `add_rescaling` transform:

```
python onnx_transforms.py --model mnist_cnn_quantized.onnx \
                          --save_model mnist_cnn_sanitized.onnx \
                          --add_rescaling --scale 0.0127 --zero-point 33
```