# ONNX integer graph proof-of-concept
A proof-of-concept for the conversion of a quantized ONN model to an integer graph of operations

## Docker setup

### Build docker image

```
./docker.sh -c 1
```

### Run docker image

```
./docker.sh
```

## Venv setup

### Create venv

```
python -m venv onnx_fp_venv
```

### Activate venv

```
source ./onnx_fp_venv/bin/activate
```

## Training of a basic MNIST model

The first step is to instantiate and train a basic MNIST model for a few epochs.

```
python train_mnist_model.py --epochs 5 --save_model
```

This should produce a model that reaches `99 %` accuracy, and save it as `mnist_cnn.pt`.

Note: the model is saved as a state_dict, so it cannot be instantiated without
the original model definition.

## Quantize a trained MNIST model

The next step is to statically quantize the float MNIST model to 8-bit.

```
python quantize_model.py --model mnist_cnn.pt \
                         --save_model mnist_cnn_quantized.pts
```

This will quantize the model to 8-bit and calibrate it, resulting in a model
with an equivalent accuracy (99 %).

Note that the model is calibrated with random inputs: calibrating it with inputs
from the dataset will likely improve the accuracy.

Note also that the quantization script does not use the default quantization
backend configuration, as it includes a fused operator that is not supported
by ONNX (the default config is therefore edited to remove the transformation
producing the fused operator).

The quantized model is saved as a `mnist_cnn_quantized.pts` TorchScript model
that can be used for inference or conversion to ONNX.

## Convert the quantized model to ONNX

The conversion to ONNX is rather straightforward. The only caveat is that the
batch size must be specified at conversion, as it is not dynamic.

```
python torch_to_onnx.py --model mnist_cnn_quantized.pts \
                        --save_model mnist_cnn_quantized.onnx \
                        --test-batch-size 1000
```

Once converted, the model can be used for inference on the MNIST test set.

```
python onnx_mnist.py --model mnist_cnn_quantized.onnx \
                     --test-batch-size 1000
```

## Modify the ONNX quantized model to use integer-only operation

### Sanitization

The quantized model generated by pytorch contains spurious operations.

The first transformation that needs to be applied is to sanitize the graph by
applying a built-in onnx_graphsurgeon transformation to fold constant nodes and
remove some of them.

```
python onnx_transforms.py --model mnist_cnn_quantized.onnx \
                          --save_model mnist_cnn_sanitized.onnx
```

Note: this is strictly equivalent to calling the NVIDIA `polygraphy` command-line tool
with the `surgeon sanitize` command.

### Add preprocessing to the graph

Since we only want integer operations, we need to integrate the uint8 inputs preprocessing in the graph.

The MNIST preprocessing applies the following Normalization:

```
float_inputs = (inputs / 255 - mean) / std

With:
 mean = 0.1307
 std = 0.3081
```

We can replace it with a `DequantizeLinear` operation:

```
float_inputs = scale * (inputs - zero_point)

With:
  scale = 1 / (255 * std) = 0.0127
  zero_point = round(255 * mean) = 33
```

This transformation is available through the new `add_rescaling` transform:

```
python onnx_transforms.py --model mnist_cnn_quantized.onnx \
                          --save_model mnist_cnn_rescaled.onnx \
                          --add_rescaling --scale 0.0127 --zero-point 33
```

### Remove useless Quantize/Dequantize sequences

When a Node corresponds to an operation that accepts either float or integer
input, there is no need to quantize and dequantize the outputs of the previous
operation.

Example:

```
Linear -> Quantize -> Cast -> Dequantize -> ReLU   <=>   Linear -> ReLU
```

This transformation is available using the new `prune_qdq` transform:

```
python onnx_transforms.py --model mnist_cnn_rescaled.onnx \
                          --save_model mnist_cnn_pruned.onnx \
                          --prune_qdq
```

### Fold inputs and weights scale into the layer output scale

Each operation has the following pattern:

Dequantize(inputs,           Dequantize(weights,
           i_scale,                     w_scale,
           i_zeropoint)                 w_zeropoint)
                 \              /
                     operation
                         |
                      Quantize(outputs, o_scale, o_zeropoint)

The goal of this transformation is to fold the inputs and weights scales into
the output scale:

o_scale = o_scale / (i_scale * w_scale)
i_scale = 1.0
w_scale = 1.0

Note: bias scale wich is equal to i_scale * w_scale before folding is also set to 1.0

```
python onnx_transforms.py --model mnist_cnn_pruned.onnx \
                          --save_model mnist_cnn_folded.onnx \
                          --fold_op_scales
```

### Perform bias addition as a separate operation

ConvInteger and MatMulInteger operations do not support biases.

The graph is therefore modified to remove the biases from the Conv and Linear operations and
add them as an explicit operation instead.

```
python onnx_transforms.py --model mnist_cnn_folded.onnx \
                          --save_model mnist_cnn_split.onnx \
                          --split_bias_add
```
